{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74d7eac9",
   "metadata": {},
   "source": [
    "# Part 2: Preprocess Data\n",
    "\n",
    "This notebook loads the raw form responses and preprocesses them for clustering analysis.\n",
    "\n",
    "## Input\n",
    "- `raw_form_responses.pickle`: Raw form data from Part 1\n",
    "- `form_metadata.json`: Form metadata\n",
    "\n",
    "## Output  \n",
    "- `preprocessed_data.pickle`: Cleaned and vectorized text data ready for clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff825ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for text processing\n",
    "!pip install pandas scikit-learn nltk textblob\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"âœ… Libraries imported and NLTK data downloaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7117c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from Part 1\n",
    "print(\"ðŸ“¥ Loading data from Part 1...\")\n",
    "\n",
    "try:\n",
    "    with open('raw_form_responses.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    df = data['dataframe']\n",
    "    form_structure = data['form_structure']\n",
    "    raw_responses = data['raw_responses']\n",
    "    \n",
    "    with open('form_metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "    print(f\"âœ… Data loaded successfully!\")\n",
    "    print(f\"ðŸ“Š DataFrame shape: {df.shape}\")\n",
    "    print(f\"ðŸ“‹ Form: {metadata['form_title']}\")\n",
    "    print(f\"ðŸ”¢ Total responses: {metadata['total_responses']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Data files not found. Please run Part 1 first!\")\n",
    "    raise\n",
    "\n",
    "# Display data overview\n",
    "print(f\"\\nðŸ“‚ Available columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"  {i+1}. {col}\")\n",
    "    \n",
    "display(df.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83610869",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"Clean and preprocess text data\"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def combine_text_responses(df, text_columns=None):\n",
    "    \"\"\"Combine multiple text columns into a single text for analysis\"\"\"\n",
    "    if text_columns is None:\n",
    "        # Identify text columns (exclude metadata columns)\n",
    "        text_columns = [col for col in df.columns \n",
    "                       if col not in ['response_id', 'create_time', 'last_submitted_time']]\n",
    "    \n",
    "    # Combine text from all specified columns\n",
    "    combined_texts = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text_parts = []\n",
    "        for col in text_columns:\n",
    "            if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                text_parts.append(str(row[col]))\n",
    "        \n",
    "        combined_text = ' '.join(text_parts)\n",
    "        combined_texts.append(combined_text)\n",
    "    \n",
    "    return combined_texts\n",
    "\n",
    "print(\"ðŸ”§ Text preprocessing functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c58c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify text columns for clustering\n",
    "text_columns = [col for col in df.columns \n",
    "                if col not in ['response_id', 'create_time', 'last_submitted_time']]\n",
    "\n",
    "print(f\"ðŸ” Text columns identified: {text_columns}\")\n",
    "\n",
    "# Combine and preprocess text responses\n",
    "if len(df) > 0:\n",
    "    print(f\"\\nðŸ”„ Processing {len(df)} responses...\")\n",
    "    \n",
    "    combined_texts = combine_text_responses(df, text_columns)\n",
    "    processed_texts = [preprocess_text(text) for text in combined_texts]\n",
    "    \n",
    "    # Filter out empty responses\n",
    "    non_empty_indices = [i for i, text in enumerate(processed_texts) if text.strip()]\n",
    "    processed_texts_filtered = [processed_texts[i] for i in non_empty_indices]\n",
    "    filtered_df = df.iloc[non_empty_indices].copy()\n",
    "    original_texts = [combined_texts[i] for i in non_empty_indices]\n",
    "    \n",
    "    print(f\"ðŸ“Š After filtering: {len(processed_texts_filtered)} non-empty responses\")\n",
    "    \n",
    "    # Add processed text to DataFrame\n",
    "    filtered_df['processed_text'] = processed_texts_filtered\n",
    "    filtered_df['original_combined_text'] = original_texts\n",
    "    \n",
    "    print(\"\\nðŸ“ Sample processed texts:\")\n",
    "    for i in range(min(3, len(processed_texts_filtered))):\n",
    "        print(f\"  {i+1}. Original: {original_texts[i][:100]}...\")\n",
    "        print(f\"     Processed: {processed_texts_filtered[i][:100]}...\")\n",
    "        print()\n",
    "        \n",
    "else:\n",
    "    print(\"âŒ No data available for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a486790d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF\n",
    "if len(processed_texts_filtered) > 0:\n",
    "    print(\"ðŸ”¢ Vectorizing text using TF-IDF...\")\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,  # Limit to top 1000 features\n",
    "        min_df=2,          # Ignore terms that appear in less than 2 documents\n",
    "        max_df=0.8,        # Ignore terms that appear in more than 80% of documents\n",
    "        ngram_range=(1, 2) # Include both unigrams and bigrams\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the text\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_texts_filtered)\n",
    "    \n",
    "    print(f\"ðŸ“Š TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "    print(f\"ðŸ”¤ Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
    "    \n",
    "    # Convert to dense array for clustering (if not too large)\n",
    "    if tfidf_matrix.shape[0] * tfidf_matrix.shape[1] < 100000:\n",
    "        X_dense = tfidf_matrix.toarray()\n",
    "        print(\"ðŸ“¦ Converted to dense matrix for clustering\")\n",
    "    else:\n",
    "        X_dense = tfidf_matrix\n",
    "        print(\"ðŸ“¦ Keeping sparse matrix due to size\")\n",
    "    \n",
    "    # Save preprocessed data\n",
    "    preprocessed_data = {\n",
    "        'dataframe': filtered_df,\n",
    "        'processed_texts': processed_texts_filtered,\n",
    "        'original_texts': original_texts,\n",
    "        'tfidf_matrix': X_dense,\n",
    "        'vectorizer': vectorizer,\n",
    "        'text_columns': text_columns,\n",
    "        'feature_names': vectorizer.get_feature_names_out()\n",
    "    }\n",
    "    \n",
    "    with open('preprocessed_data.pickle', 'wb') as f:\n",
    "        pickle.dump(preprocessed_data, f)\n",
    "    \n",
    "    print(\"âœ… Preprocessed data saved!\")\n",
    "    print(\"ðŸ“ File created: preprocessed_data.pickle\")\n",
    "    print(f\"ðŸ“Š Ready for clustering: {len(processed_texts_filtered)} responses\")\n",
    "    print(f\"ðŸ”¤ Features: {tfidf_matrix.shape[1]} TF-IDF terms\")\n",
    "    \n",
    "    # Show sample features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nðŸ” Sample features: {feature_names[:10]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"âŒ No processed texts available for vectorization.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
