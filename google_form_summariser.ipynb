{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c49154e",
   "metadata": {},
   "source": [
    "# Google Form Response Summarizer with HDBSCAN Clustering\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Connect to Google Forms API to fetch form responses\n",
    "2. Preprocess the text responses for analysis\n",
    "3. Use HDBSCAN clustering to group similar responses\n",
    "4. Generate summaries for each cluster to understand common themes\n",
    "\n",
    "## Prerequisites\n",
    "- Google Cloud Project with Forms API enabled\n",
    "- Service account credentials JSON file\n",
    "- Form ID of the Google Form you want to analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d8fb1",
   "metadata": {},
   "source": [
    "## 1. Install and Import Required Libraries\n",
    "\n",
    "First, we'll install the necessary packages for connecting to Google Forms API and performing clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b399cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install google-auth google-auth-oauthlib google-api-python-client\n",
    "!pip install hdbscan pandas scikit-learn matplotlib seaborn plotly\n",
    "!pip install nltk textblob wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25634b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Google API imports\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "# Text processing and clustering imports\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Other utilities\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f74435d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download required NLTK data\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "print(\"NLTK data downloaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805ae72",
   "metadata": {},
   "source": [
    "## 2. Authenticate and Connect to Google Forms API\n",
    "\n",
    "Set up authentication credentials and connect to the Google Forms API. \n",
    "You'll need to:\n",
    "1. Create a Google Cloud Project\n",
    "2. Enable the Google Forms API\n",
    "3. Download service account credentials or use OAuth2 flow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16eaec57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "SCOPES = ['https://www.googleapis.com/auth/forms.responses.readonly',\n",
    "          'https://www.googleapis.com/auth/forms.body.readonly']\n",
    "\n",
    "# Update this with your form ID\n",
    "FORM_ID = 'YOUR_FORM_ID_HERE'  # Replace with actual form ID\n",
    "\n",
    "def authenticate_google_api():\n",
    "    \"\"\"\n",
    "    Authenticate with Google API using OAuth2 or service account\n",
    "    \"\"\"\n",
    "    creds = None\n",
    "    \n",
    "    # The file token.json stores the user's access and refresh tokens.\n",
    "    if os.path.exists('token.json'):\n",
    "        creds = Credentials.from_authorized_user_file('token.json', SCOPES)\n",
    "    \n",
    "    # If there are no (valid) credentials available, let the user log in.\n",
    "    if not creds or not creds.valid:\n",
    "        if creds and creds.expired and creds.refresh_token:\n",
    "            creds.refresh(Request())\n",
    "        else:\n",
    "            # Update path to your credentials file\n",
    "            flow = InstalledAppFlow.from_client_secrets_file(\n",
    "                'credentials.json', SCOPES)  # Download from Google Cloud Console\n",
    "            creds = flow.run_local_server(port=0)\n",
    "        \n",
    "        # Save the credentials for the next run\n",
    "        with open('token.json', 'w') as token:\n",
    "            token.write(creds.to_json())\n",
    "    \n",
    "    return creds\n",
    "\n",
    "# Authenticate and build service\n",
    "try:\n",
    "    creds = authenticate_google_api()\n",
    "    service = build('forms', 'v1', credentials=creds)\n",
    "    print(\"Successfully authenticated with Google Forms API!\")\n",
    "except Exception as e:\n",
    "    print(f\"Authentication error: {e}\")\n",
    "    print(\"Please ensure you have credentials.json file in the current directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db59b9b6",
   "metadata": {},
   "source": [
    "## 3. Fetch Google Form Responses\n",
    "\n",
    "Retrieve responses from the specified Google Form and convert them into a pandas DataFrame for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd52abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_form_structure(service, form_id):\n",
    "    \"\"\"\n",
    "    Get the form structure to understand question types and IDs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        form = service.forms().get(formId=form_id).execute()\n",
    "        return form\n",
    "    except HttpError as e:\n",
    "        print(f\"Error getting form structure: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_form_responses(service, form_id):\n",
    "    \"\"\"\n",
    "    Fetch all responses from the Google Form\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get form responses\n",
    "        responses = service.forms().responses().list(formId=form_id).execute()\n",
    "        return responses.get('responses', [])\n",
    "    except HttpError as e:\n",
    "        print(f\"Error fetching responses: {e}\")\n",
    "        return []\n",
    "\n",
    "def process_responses_to_dataframe(form_structure, responses):\n",
    "    \"\"\"\n",
    "    Convert form responses to a pandas DataFrame\n",
    "    \"\"\"\n",
    "    if not responses:\n",
    "        print(\"No responses found\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Extract question titles and IDs\n",
    "    questions = {}\n",
    "    if 'items' in form_structure:\n",
    "        for item in form_structure['items']:\n",
    "            if 'questionItem' in item:\n",
    "                question_id = item['questionItem']['question']['questionId']\n",
    "                title = item['title']\n",
    "                questions[question_id] = title\n",
    "    \n",
    "    # Process responses\n",
    "    processed_data = []\n",
    "    \n",
    "    for response in responses:\n",
    "        response_data = {\n",
    "            'response_id': response.get('responseId', ''),\n",
    "            'create_time': response.get('createTime', ''),\n",
    "            'last_submitted_time': response.get('lastSubmittedTime', '')\n",
    "        }\n",
    "        \n",
    "        # Extract answers\n",
    "        if 'answers' in response:\n",
    "            for question_id, answer in response['answers'].items():\n",
    "                question_title = questions.get(question_id, f'Question_{question_id}')\n",
    "                \n",
    "                # Handle different answer types\n",
    "                if 'textAnswers' in answer:\n",
    "                    text_values = [ta.get('value', '') for ta in answer['textAnswers']['answers']]\n",
    "                    response_data[question_title] = '; '.join(text_values)\n",
    "                elif 'fileUploadAnswers' in answer:\n",
    "                    response_data[question_title] = 'File uploaded'\n",
    "                else:\n",
    "                    response_data[question_title] = str(answer)\n",
    "        \n",
    "        processed_data.append(response_data)\n",
    "    \n",
    "    return pd.DataFrame(processed_data)\n",
    "\n",
    "# Fetch form data\n",
    "print(\"Fetching form structure...\")\n",
    "form_structure = get_form_structure(service, FORM_ID)\n",
    "\n",
    "if form_structure:\n",
    "    print(f\"Form Title: {form_structure.get('info', {}).get('title', 'Unknown')}\")\n",
    "    print(f\"Form Description: {form_structure.get('info', {}).get('description', 'No description')}\")\n",
    "    \n",
    "    print(\"\\nFetching responses...\")\n",
    "    responses = get_form_responses(service, FORM_ID)\n",
    "    \n",
    "    print(f\"Found {len(responses)} responses\")\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    df = process_responses_to_dataframe(form_structure, responses)\n",
    "    print(f\"DataFrame shape: {df.shape}\")\n",
    "    \n",
    "    # Display first few responses\n",
    "    print(\"\\nFirst 5 responses:\")\n",
    "    display(df.head())\n",
    "else:\n",
    "    print(\"Could not fetch form structure. Please check your FORM_ID and permissions.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42187c13",
   "metadata": {},
   "source": [
    "## 4. Preprocess Responses for Clustering\n",
    "\n",
    "Clean and preprocess text responses to prepare them for clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be694c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Clean and preprocess text data\n",
    "    \"\"\"\n",
    "    if pd.isna(text) or text == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [token for token in tokens if token not in stop_words and len(token) > 2]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def combine_text_responses(df, text_columns=None):\n",
    "    \"\"\"\n",
    "    Combine multiple text columns into a single text for analysis\n",
    "    \"\"\"\n",
    "    if text_columns is None:\n",
    "        # Identify text columns (exclude metadata columns)\n",
    "        text_columns = [col for col in df.columns \n",
    "                       if col not in ['response_id', 'create_time', 'last_submitted_time']]\n",
    "    \n",
    "    # Combine text from all specified columns\n",
    "    combined_texts = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text_parts = []\n",
    "        for col in text_columns:\n",
    "            if pd.notna(row[col]) and str(row[col]).strip():\n",
    "                text_parts.append(str(row[col]))\n",
    "        \n",
    "        combined_text = ' '.join(text_parts)\n",
    "        combined_texts.append(combined_text)\n",
    "    \n",
    "    return combined_texts\n",
    "\n",
    "# Analyze the DataFrame structure\n",
    "print(\"DataFrame columns:\")\n",
    "for i, col in enumerate(df.columns):\n",
    "    print(f\"{i+1}. {col}\")\n",
    "\n",
    "# Identify text columns for clustering\n",
    "text_columns = [col for col in df.columns \n",
    "                if col not in ['response_id', 'create_time', 'last_submitted_time']]\n",
    "\n",
    "print(f\"\\nText columns identified for analysis: {text_columns}\")\n",
    "\n",
    "# Combine and preprocess text responses\n",
    "if len(df) > 0:\n",
    "    combined_texts = combine_text_responses(df, text_columns)\n",
    "    \n",
    "    print(f\"\\nPreprocessing {len(combined_texts)} responses...\")\n",
    "    processed_texts = [preprocess_text(text) for text in combined_texts]\n",
    "    \n",
    "    # Filter out empty responses\n",
    "    non_empty_indices = [i for i, text in enumerate(processed_texts) if text.strip()]\n",
    "    processed_texts = [processed_texts[i] for i in non_empty_indices]\n",
    "    filtered_df = df.iloc[non_empty_indices].copy()\n",
    "    \n",
    "    print(f\"After filtering empty responses: {len(processed_texts)} responses remain\")\n",
    "    \n",
    "    # Add processed text to DataFrame\n",
    "    filtered_df['processed_text'] = processed_texts\n",
    "    \n",
    "    print(\"\\nSample processed texts:\")\n",
    "    for i in range(min(3, len(processed_texts))):\n",
    "        print(f\"{i+1}. Original: {combined_texts[non_empty_indices[i]][:100]}...\")\n",
    "        print(f\"   Processed: {processed_texts[i][:100]}...\")\n",
    "        print()\n",
    "else:\n",
    "    print(\"No data available for processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b995d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize text using TF-IDF\n",
    "if len(processed_texts) > 0:\n",
    "    print(\"Vectorizing text using TF-IDF...\")\n",
    "    \n",
    "    # Create TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        max_features=1000,  # Limit to top 1000 features\n",
    "        min_df=2,          # Ignore terms that appear in less than 2 documents\n",
    "        max_df=0.8,        # Ignore terms that appear in more than 80% of documents\n",
    "        ngram_range=(1, 2) # Include both unigrams and bigrams\n",
    "    )\n",
    "    \n",
    "    # Fit and transform the text\n",
    "    tfidf_matrix = vectorizer.fit_transform(processed_texts)\n",
    "    \n",
    "    print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "    print(f\"Number of features: {len(vectorizer.get_feature_names_out())}\")\n",
    "    \n",
    "    # Convert to dense array for clustering (if not too large)\n",
    "    if tfidf_matrix.shape[0] * tfidf_matrix.shape[1] < 100000:\n",
    "        X = tfidf_matrix.toarray()\n",
    "    else:\n",
    "        X = tfidf_matrix  # Keep sparse for large datasets\n",
    "    \n",
    "    print(\"Text vectorization completed!\")\n",
    "    \n",
    "    # Show most important features\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    print(f\"\\nSample features: {feature_names[:20]}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No processed texts available for vectorization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48565d6",
   "metadata": {},
   "source": [
    "## 5. Apply HDBSCAN Clustering\n",
    "\n",
    "Use HDBSCAN to cluster the preprocessed responses and identify groups of similar feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb6edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN clustering\n",
    "if len(processed_texts) > 0:\n",
    "    print(\"Applying HDBSCAN clustering...\")\n",
    "    \n",
    "    # Convert sparse matrix to dense if needed\n",
    "    if hasattr(X, 'toarray'):\n",
    "        X_dense = X.toarray()\n",
    "    else:\n",
    "        X_dense = X\n",
    "    \n",
    "    # Configure HDBSCAN parameters\n",
    "    min_cluster_size = max(2, len(processed_texts) // 10)  # Adaptive minimum cluster size\n",
    "    min_samples = max(1, min_cluster_size // 2)           # Minimum samples\n",
    "    \n",
    "    print(f\"HDBSCAN parameters:\")\n",
    "    print(f\"- min_cluster_size: {min_cluster_size}\")\n",
    "    print(f\"- min_samples: {min_samples}\")\n",
    "    \n",
    "    # Create and fit HDBSCAN clusterer\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=min_cluster_size,\n",
    "        min_samples=min_samples,\n",
    "        metric='euclidean',\n",
    "        cluster_selection_method='eom'\n",
    "    )\n",
    "    \n",
    "    # Fit the model\n",
    "    cluster_labels = clusterer.fit_predict(X_dense)\n",
    "    \n",
    "    # Add cluster labels to DataFrame\n",
    "    filtered_df['cluster'] = cluster_labels\n",
    "    \n",
    "    # Analyze clustering results\n",
    "    n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "    n_noise = list(cluster_labels).count(-1)\n",
    "    \n",
    "    print(f\"\\nClustering Results:\")\n",
    "    print(f\"- Number of clusters: {n_clusters}\")\n",
    "    print(f\"- Number of noise points: {n_noise}\")\n",
    "    print(f\"- Percentage of noise: {n_noise/len(cluster_labels)*100:.1f}%\")\n",
    "    \n",
    "    # Cluster size distribution\n",
    "    cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "    print(f\"\\nCluster size distribution:\")\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        if cluster_id == -1:\n",
    "            print(f\"  Noise: {count} responses\")\n",
    "        else:\n",
    "            print(f\"  Cluster {cluster_id}: {count} responses\")\n",
    "    \n",
    "    # Calculate clustering quality metrics\n",
    "    if n_clusters > 1:\n",
    "        # Silhouette score (excluding noise points)\n",
    "        non_noise_mask = cluster_labels != -1\n",
    "        if np.sum(non_noise_mask) > 1:\n",
    "            silhouette_avg = silhouette_score(X_dense[non_noise_mask], \n",
    "                                            cluster_labels[non_noise_mask])\n",
    "            print(f\"\\nSilhouette Score: {silhouette_avg:.3f}\")\n",
    "    \n",
    "else:\n",
    "    print(\"No data available for clustering.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd09048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "if len(processed_texts) > 0 and n_clusters > 0:\n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Cluster size distribution\n",
    "    cluster_counts_viz = cluster_counts[cluster_counts.index != -1]  # Exclude noise\n",
    "    axes[0, 0].bar(range(len(cluster_counts_viz)), cluster_counts_viz.values)\n",
    "    axes[0, 0].set_title('Cluster Size Distribution')\n",
    "    axes[0, 0].set_xlabel('Cluster ID')\n",
    "    axes[0, 0].set_ylabel('Number of Responses')\n",
    "    axes[0, 0].set_xticks(range(len(cluster_counts_viz)))\n",
    "    axes[0, 0].set_xticklabels(cluster_counts_viz.index)\n",
    "    \n",
    "    # 2. Cluster membership overview\n",
    "    cluster_data = pd.DataFrame({'cluster': cluster_labels})\n",
    "    cluster_summary = cluster_data['cluster'].value_counts().sort_index()\n",
    "    \n",
    "    # Pie chart for cluster distribution\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_summary)))\n",
    "    labels = [f'Cluster {i}' if i != -1 else 'Noise' for i in cluster_summary.index]\n",
    "    \n",
    "    axes[0, 1].pie(cluster_summary.values, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "    axes[0, 1].set_title('Cluster Distribution')\\n    \n",
    "    # 3. Cluster confidence scores (HDBSCAN probabilities)\n",
    "    if hasattr(clusterer, 'probabilities_'):\n",
    "        axes[1, 0].hist(clusterer.probabilities_, bins=30, alpha=0.7)\n",
    "        axes[1, 0].set_title('Cluster Membership Probabilities')\n",
    "        axes[1, 0].set_xlabel('Probability')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Probabilities not available', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Cluster Probabilities')\n",
    "    \n",
    "    # 4. Response length by cluster\n",
    "    response_lengths = [len(text.split()) for text in processed_texts]\n",
    "    cluster_df_viz = pd.DataFrame({\n",
    "        'cluster': cluster_labels,\n",
    "        'length': response_lengths\n",
    "    })\n",
    "    \n",
    "    for cluster_id in sorted(set(cluster_labels)):\n",
    "        if cluster_id != -1:\n",
    "            cluster_lengths = cluster_df_viz[cluster_df_viz['cluster'] == cluster_id]['length']\n",
    "            axes[1, 1].hist(cluster_lengths, alpha=0.6, label=f'Cluster {cluster_id}', bins=15)\n",
    "    \n",
    "    axes[1, 1].set_title('Response Length Distribution by Cluster')\n",
    "    axes[1, 1].set_xlabel('Number of Words')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"No clusters found or insufficient data for visualization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b50ef",
   "metadata": {},
   "source": [
    "## 6. Summarize Clustered Responses\n",
    "\n",
    "Generate summaries and insights for each cluster to understand common themes and patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e2edca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster(cluster_df, cluster_id, vectorizer, X_dense):\n",
    "    \"\"\"\n",
    "    Analyze a specific cluster and generate insights\n",
    "    \"\"\"\n",
    "    if cluster_id == -1:\n",
    "        return {\"cluster_id\": -1, \"name\": \"Noise\", \"size\": len(cluster_df)}\n",
    "    \n",
    "    cluster_texts = cluster_df['processed_text'].tolist()\n",
    "    original_texts = []\n",
    "    \n",
    "    # Get original combined texts\n",
    "    for idx in cluster_df.index:\n",
    "        original_row = df.iloc[idx]\n",
    "        original_parts = []\n",
    "        for col in text_columns:\n",
    "            if pd.notna(original_row[col]) and str(original_row[col]).strip():\n",
    "                original_parts.append(str(original_row[col]))\n",
    "        original_texts.append(' '.join(original_parts))\n",
    "    \n",
    "    # Get most important terms for this cluster\n",
    "    cluster_indices = cluster_df.index.tolist()\n",
    "    cluster_vectors = X_dense[cluster_indices]\n",
    "    \n",
    "    # Calculate mean TF-IDF values for cluster\n",
    "    mean_tfidf = np.mean(cluster_vectors, axis=0)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get top terms\n",
    "    top_indices = np.argsort(mean_tfidf)[-10:][::-1]\n",
    "    top_terms = [(feature_names[i], mean_tfidf[i]) for i in top_indices if mean_tfidf[i] > 0]\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiments = [TextBlob(text).sentiment.polarity for text in original_texts]\n",
    "    avg_sentiment = np.mean(sentiments)\n",
    "    \n",
    "    # Response length statistics\n",
    "    lengths = [len(text.split()) for text in original_texts]\n",
    "    \n",
    "    return {\n",
    "        \"cluster_id\": cluster_id,\n",
    "        \"size\": len(cluster_df),\n",
    "        \"top_terms\": top_terms,\n",
    "        \"avg_sentiment\": avg_sentiment,\n",
    "        \"avg_length\": np.mean(lengths),\n",
    "        \"sample_responses\": original_texts[:3],  # First 3 responses as examples\n",
    "        \"processed_sample\": cluster_texts[:3]\n",
    "    }\n",
    "\n",
    "# Analyze each cluster\n",
    "if len(processed_texts) > 0 and n_clusters > 0:\n",
    "    print(\"Analyzing clusters...\")\n",
    "    \n",
    "    cluster_analyses = []\n",
    "    \n",
    "    for cluster_id in sorted(set(cluster_labels)):\n",
    "        cluster_mask = filtered_df['cluster'] == cluster_id\n",
    "        cluster_data = filtered_df[cluster_mask]\n",
    "        \n",
    "        analysis = analyze_cluster(cluster_data, cluster_id, vectorizer, X_dense)\n",
    "        cluster_analyses.append(analysis)\n",
    "        \n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"CLUSTER {cluster_id} ANALYSIS\")\n",
    "        print(f\"{'='*50}\")\n",
    "        \n",
    "        if cluster_id == -1:\n",
    "            print(f\"Type: Noise/Outliers\")\n",
    "            print(f\"Size: {analysis['size']} responses\")\n",
    "        else:\n",
    "            print(f\"Size: {analysis['size']} responses\")\n",
    "            print(f\"Average Sentiment: {analysis['avg_sentiment']:.3f} ({'Positive' if analysis['avg_sentiment'] > 0.1 else 'Negative' if analysis['avg_sentiment'] < -0.1 else 'Neutral'})\")\n",
    "            print(f\"Average Response Length: {analysis['avg_length']:.1f} words\")\n",
    "            \n",
    "            print(f\"\\nTop Terms:\")\n",
    "            for term, score in analysis['top_terms'][:5]:\n",
    "                print(f\"  - {term}: {score:.3f}\")\n",
    "            \n",
    "            print(f\"\\nSample Responses:\")\n",
    "            for i, response in enumerate(analysis['sample_responses']):\n",
    "                print(f\"  {i+1}. {response[:150]}{'...' if len(response) > 150 else ''}\")\n",
    "    \n",
    "    # Create summary DataFrame\n",
    "    summary_data = []\n",
    "    for analysis in cluster_analyses:\n",
    "        if analysis['cluster_id'] != -1:\n",
    "            summary_data.append({\n",
    "                'Cluster': analysis['cluster_id'],\n",
    "                'Size': analysis['size'],\n",
    "                'Avg_Sentiment': round(analysis['avg_sentiment'], 3),\n",
    "                'Avg_Length': round(analysis['avg_length'], 1),\n",
    "                'Top_Terms': ', '.join([term for term, _ in analysis['top_terms'][:3]])\n",
    "            })\n",
    "    \n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(\"CLUSTER SUMMARY TABLE\")\n",
    "    print(f\"{'='*50}\")\n",
    "    display(summary_df)\n",
    "    \n",
    "else:\n",
    "    print(\"No clusters available for analysis.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77888f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds for each cluster\n",
    "if len(processed_texts) > 0 and n_clusters > 0:\n",
    "    print(\"Generating word clouds for clusters...\")\n",
    "    \n",
    "    # Calculate number of rows needed for subplots\n",
    "    n_clusters_to_show = min(n_clusters, 6)  # Show up to 6 clusters\n",
    "    n_cols = min(3, n_clusters_to_show)\n",
    "    n_rows = (n_clusters_to_show + n_cols - 1) // n_cols\n",
    "    \n",
    "    if n_clusters_to_show > 0:\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if n_clusters_to_show == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        cluster_counter = 0\n",
    "        \n",
    "        for analysis in cluster_analyses:\n",
    "            if analysis['cluster_id'] != -1 and cluster_counter < n_clusters_to_show:\n",
    "                cluster_id = analysis['cluster_id']\n",
    "                cluster_mask = filtered_df['cluster'] == cluster_id\n",
    "                cluster_texts = filtered_df[cluster_mask]['processed_text'].tolist()\n",
    "                \n",
    "                # Combine all text in cluster\n",
    "                cluster_text = ' '.join(cluster_texts)\n",
    "                \n",
    "                if cluster_text.strip():  # Only create wordcloud if there's text\n",
    "                    try:\n",
    "                        wordcloud = WordCloud(\n",
    "                            width=400, \n",
    "                            height=300, \n",
    "                            background_color='white',\n",
    "                            max_words=50,\n",
    "                            colormap='viridis'\n",
    "                        ).generate(cluster_text)\n",
    "                        \n",
    "                        axes[cluster_counter].imshow(wordcloud, interpolation='bilinear')\n",
    "                        axes[cluster_counter].set_title(f'Cluster {cluster_id} Word Cloud\\n({analysis[\"size\"]} responses)')\n",
    "                        axes[cluster_counter].axis('off')\n",
    "                    except ValueError:\n",
    "                        axes[cluster_counter].text(0.5, 0.5, f'Cluster {cluster_id}\\nInsufficient text for word cloud', \n",
    "                                                 ha='center', va='center', transform=axes[cluster_counter].transAxes)\n",
    "                        axes[cluster_counter].set_title(f'Cluster {cluster_id}')\n",
    "                \n",
    "                cluster_counter += 1\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(cluster_counter, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    else:\n",
    "        print(\"No clusters available for word cloud generation.\")\n",
    "\n",
    "else:\n",
    "    print(\"No data available for word cloud generation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50eb7640",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to CSV\n",
    "if len(processed_texts) > 0:\n",
    "    print(\"Exporting results...\")\n",
    "    \n",
    "    # Create detailed results DataFrame\n",
    "    export_df = filtered_df.copy()\n",
    "    \n",
    "    # Add cluster information\n",
    "    cluster_names = {}\n",
    "    for analysis in cluster_analyses:\n",
    "        if analysis['cluster_id'] != -1:\n",
    "            top_terms = [term for term, _ in analysis['top_terms'][:2]]\n",
    "            cluster_names[analysis['cluster_id']] = f\"Cluster_{analysis['cluster_id']}_{'_'.join(top_terms)}\"\n",
    "        else:\n",
    "            cluster_names[analysis['cluster_id']] = \"Noise\"\n",
    "    \n",
    "    export_df['cluster_name'] = export_df['cluster'].map(cluster_names)\n",
    "    \n",
    "    # Add sentiment scores\n",
    "    original_combined_texts = combine_text_responses(export_df, text_columns)\n",
    "    export_df['sentiment_score'] = [TextBlob(text).sentiment.polarity for text in original_combined_texts]\n",
    "    export_df['response_length'] = [len(text.split()) for text in original_combined_texts]\n",
    "    \n",
    "    # Export to CSV\n",
    "    output_filename = 'google_form_clustering_results.csv'\n",
    "    export_df.to_csv(output_filename, index=False)\n",
    "    print(f\"Results exported to: {output_filename}\")\n",
    "    \n",
    "    # Create summary report\n",
    "    report = f\\\"\\\"\\\"\n",
    "GOOGLE FORM CLUSTERING ANALYSIS REPORT\n",
    "=====================================\n",
    "\n",
    "Dataset Overview:\n",
    "- Total responses analyzed: {len(export_df)}\n",
    "- Number of clusters found: {n_clusters}\n",
    "- Noise points: {n_noise} ({n_noise/len(cluster_labels)*100:.1f}%)\n",
    "\n",
    "Cluster Summary:\n",
    "\\\"\\\"\\\"\\n    \\n    for analysis in cluster_analyses:\\n        if analysis['cluster_id'] != -1:\\n            report += f\\\"\\\"\\\"\\n\\nCluster {analysis['cluster_id']}:\\n- Size: {analysis['size']} responses ({analysis['size']/len(export_df)*100:.1f}%)\\n- Average sentiment: {analysis['avg_sentiment']:.3f}\\n- Average length: {analysis['avg_length']:.1f} words\\n- Key themes: {', '.join([term for term, _ in analysis['top_terms'][:3]])}\\n\\\"\\\"\\\"\\n    \\n    # Save report\\n    report_filename = 'clustering_analysis_report.txt'\\n    with open(report_filename, 'w') as f:\\n        f.write(report)\\n    \\n    print(f\\\"Analysis report saved to: {report_filename}\\\")\\n    print(\\\"\\\\nAnalysis completed successfully!\\\")\\n    \\nelse:\\n    print(\\\"No data available for export.\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517bbcf8",
   "metadata": {},
   "source": [
    "## Usage Instructions and Next Steps\n",
    "\n",
    "### How to Use This Notebook:\n",
    "\n",
    "1. **Setup Google API Credentials:**\n",
    "   - Go to [Google Cloud Console](https://console.cloud.google.com/)\n",
    "   - Create a new project or select existing one\n",
    "   - Enable the Google Forms API\n",
    "   - Create credentials (OAuth 2.0 or Service Account)\n",
    "   - Download the credentials file as `credentials.json`\n",
    "\n",
    "2. **Configure the Form ID:**\n",
    "   - Replace `YOUR_FORM_ID_HERE` with your actual Google Form ID\n",
    "   - The Form ID can be found in the form URL: `https://docs.google.com/forms/d/{FORM_ID}/edit`\n",
    "\n",
    "3. **Run the Analysis:**\n",
    "   - Execute all cells in order\n",
    "   - The first run will prompt for Google authentication\n",
    "   - Results will be saved as CSV and text report files\n",
    "\n",
    "### Interpreting Results:\n",
    "\n",
    "- **Clusters**: Groups of similar responses based on content\n",
    "- **Noise Points**: Responses that don't fit well into any cluster\n",
    "- **Sentiment Scores**: Range from -1 (negative) to +1 (positive)\n",
    "- **Top Terms**: Most characteristic words for each cluster\n",
    "\n",
    "### Customization Options:\n",
    "\n",
    "- Adjust `min_cluster_size` in HDBSCAN for different cluster granularity\n",
    "- Modify TF-IDF parameters for different text processing\n",
    "- Change the number of top terms displayed\n",
    "- Add custom text preprocessing steps\n",
    "\n",
    "### Potential Extensions:\n",
    "\n",
    "- Topic modeling with LDA or BERTopic\n",
    "- Sentiment analysis with advanced models\n",
    "- Interactive visualizations with Plotly\n",
    "- Automatic cluster labeling with GPT\n",
    "- Real-time analysis with form webhooks"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
