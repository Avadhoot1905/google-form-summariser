{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d913c5a3",
   "metadata": {},
   "source": [
    "# Part 3: Apply HDBSCAN Clustering\n",
    "\n",
    "This notebook applies HDBSCAN clustering to the preprocessed text data.\n",
    "\n",
    "## Input\n",
    "- `preprocessed_data.pickle`: Vectorized text data from Part 2\n",
    "\n",
    "## Output\n",
    "- `clustered_data.pickle`: Data with cluster assignments and clustering results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c64f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for clustering\n",
    "!pip install hdbscan scikit-learn matplotlib seaborn pandas numpy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "from sklearn.metrics import silhouette_score\n",
    "import hdbscan\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✅ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa0e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preprocessed data from Part 2\n",
    "print(\"📥 Loading preprocessed data from Part 2...\")\n",
    "\n",
    "try:\n",
    "    with open('preprocessed_data.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    df = data['dataframe']\n",
    "    processed_texts = data['processed_texts']\n",
    "    original_texts = data['original_texts']\n",
    "    X = data['tfidf_matrix']\n",
    "    vectorizer = data['vectorizer']\n",
    "    text_columns = data['text_columns']\n",
    "    feature_names = data['feature_names']\n",
    "    \n",
    "    print(f\"✅ Data loaded successfully!\")\n",
    "    print(f\"📊 Data shape: {X.shape}\")\n",
    "    print(f\"🔢 Number of responses: {len(processed_texts)}\")\n",
    "    print(f\"🔤 Number of features: {len(feature_names)}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"❌ Preprocessed data file not found. Please run Part 2 first!\")\n",
    "    raise\n",
    "\n",
    "# Convert sparse matrix to dense if needed\n",
    "if hasattr(X, 'toarray'):\n",
    "    X_dense = X.toarray()\n",
    "    print(\"📦 Converted sparse matrix to dense for clustering\")\n",
    "else:\n",
    "    X_dense = X\n",
    "    print(\"📦 Using dense matrix for clustering\")\n",
    "\n",
    "print(f\"\\n🎯 Ready for clustering with {X_dense.shape[0]} samples and {X_dense.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b70926d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply HDBSCAN clustering\n",
    "print(\"🎯 Applying HDBSCAN clustering...\")\n",
    "\n",
    "# Configure HDBSCAN parameters\n",
    "min_cluster_size = max(2, len(processed_texts) // 10)  # Adaptive minimum cluster size\n",
    "min_samples = max(1, min_cluster_size // 2)           # Minimum samples\n",
    "\n",
    "print(f\"⚙️ HDBSCAN parameters:\")\n",
    "print(f\"   - min_cluster_size: {min_cluster_size}\")\n",
    "print(f\"   - min_samples: {min_samples}\")\n",
    "\n",
    "# Create and fit HDBSCAN clusterer\n",
    "clusterer = hdbscan.HDBSCAN(\n",
    "    min_cluster_size=min_cluster_size,\n",
    "    min_samples=min_samples,\n",
    "    metric='euclidean',\n",
    "    cluster_selection_method='eom'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "cluster_labels = clusterer.fit_predict(X_dense)\n",
    "\n",
    "# Add cluster labels to DataFrame\n",
    "df['cluster'] = cluster_labels\n",
    "\n",
    "# Analyze clustering results\n",
    "n_clusters = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "n_noise = list(cluster_labels).count(-1)\n",
    "\n",
    "print(f\"\\n📊 Clustering Results:\")\n",
    "print(f\"   🏷️  Number of clusters: {n_clusters}\")\n",
    "print(f\"   🔇 Number of noise points: {n_noise}\")\n",
    "print(f\"   📈 Percentage of noise: {n_noise/len(cluster_labels)*100:.1f}%\")\n",
    "\n",
    "# Cluster size distribution\n",
    "cluster_counts = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "print(f\"\\n📊 Cluster size distribution:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    if cluster_id == -1:\n",
    "        print(f\"   🔇 Noise: {count} responses\")\n",
    "    else:\n",
    "        print(f\"   🏷️  Cluster {cluster_id}: {count} responses\")\n",
    "\n",
    "# Calculate clustering quality metrics\n",
    "if n_clusters > 1:\n",
    "    # Silhouette score (excluding noise points)\n",
    "    non_noise_mask = cluster_labels != -1\n",
    "    if np.sum(non_noise_mask) > 1:\n",
    "        silhouette_avg = silhouette_score(X_dense[non_noise_mask], \n",
    "                                        cluster_labels[non_noise_mask])\n",
    "        print(f\"\\n📏 Silhouette Score: {silhouette_avg:.3f}\")\n",
    "    else:\n",
    "        print(f\"\\n📏 Silhouette Score: Cannot calculate (insufficient non-noise samples)\")\n",
    "else:\n",
    "    print(f\"\\n📏 Silhouette Score: Cannot calculate (insufficient clusters)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a7f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clustering results\n",
    "if n_clusters > 0:\n",
    "    print(\"📊 Creating visualizations...\")\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # 1. Cluster size distribution\n",
    "    cluster_counts_viz = cluster_counts[cluster_counts.index != -1]  # Exclude noise\n",
    "    if len(cluster_counts_viz) > 0:\n",
    "        axes[0, 0].bar(range(len(cluster_counts_viz)), cluster_counts_viz.values)\n",
    "        axes[0, 0].set_title('Cluster Size Distribution')\n",
    "        axes[0, 0].set_xlabel('Cluster ID')\n",
    "        axes[0, 0].set_ylabel('Number of Responses')\n",
    "        axes[0, 0].set_xticks(range(len(cluster_counts_viz)))\n",
    "        axes[0, 0].set_xticklabels(cluster_counts_viz.index)\n",
    "    \n",
    "    # 2. Cluster membership pie chart\n",
    "    cluster_summary = pd.Series(cluster_labels).value_counts().sort_index()\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(cluster_summary)))\n",
    "    labels = [f'Cluster {i}' if i != -1 else 'Noise' for i in cluster_summary.index]\n",
    "    \n",
    "    axes[0, 1].pie(cluster_summary.values, labels=labels, autopct='%1.1f%%', colors=colors)\n",
    "    axes[0, 1].set_title('Cluster Distribution')\n",
    "    \n",
    "    # 3. Cluster membership probabilities\n",
    "    if hasattr(clusterer, 'probabilities_'):\n",
    "        axes[1, 0].hist(clusterer.probabilities_, bins=30, alpha=0.7)\n",
    "        axes[1, 0].set_title('Cluster Membership Probabilities')\n",
    "        axes[1, 0].set_xlabel('Probability')\n",
    "        axes[1, 0].set_ylabel('Frequency')\n",
    "    else:\n",
    "        axes[1, 0].text(0.5, 0.5, 'Probabilities not available', \n",
    "                       ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "        axes[1, 0].set_title('Cluster Probabilities')\n",
    "    \n",
    "    # 4. Response length by cluster\n",
    "    response_lengths = [len(text.split()) for text in processed_texts]\n",
    "    cluster_df_viz = pd.DataFrame({\n",
    "        'cluster': cluster_labels,\n",
    "        'length': response_lengths\n",
    "    })\n",
    "    \n",
    "    for cluster_id in sorted(set(cluster_labels)):\n",
    "        if cluster_id != -1:\n",
    "            cluster_lengths = cluster_df_viz[cluster_df_viz['cluster'] == cluster_id]['length']\n",
    "            if len(cluster_lengths) > 0:\n",
    "                axes[1, 1].hist(cluster_lengths, alpha=0.6, label=f'Cluster {cluster_id}', bins=15)\n",
    "    \n",
    "    axes[1, 1].set_title('Response Length Distribution by Cluster')\n",
    "    axes[1, 1].set_xlabel('Number of Words')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    if n_clusters > 0:\n",
    "        axes[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️ No clusters found - skipping visualizations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b52b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save clustering results for Part 4\n",
    "clustered_data = {\n",
    "    'dataframe': df,\n",
    "    'cluster_labels': cluster_labels,\n",
    "    'clusterer': clusterer,\n",
    "    'processed_texts': processed_texts,\n",
    "    'original_texts': original_texts,\n",
    "    'vectorizer': vectorizer,\n",
    "    'tfidf_matrix': X_dense,\n",
    "    'text_columns': text_columns,\n",
    "    'feature_names': feature_names,\n",
    "    'n_clusters': n_clusters,\n",
    "    'n_noise': n_noise,\n",
    "    'cluster_counts': cluster_counts\n",
    "}\n",
    "\n",
    "with open('clustered_data.pickle', 'wb') as f:\n",
    "    pickle.dump(clustered_data, f)\n",
    "\n",
    "print(\"✅ Clustering results saved!\")\n",
    "print(\"📁 File created: clustered_data.pickle\")\n",
    "print(f\"🎯 Successfully clustered {len(processed_texts)} responses into {n_clusters} clusters\")\n",
    "\n",
    "if n_clusters > 0:\n",
    "    print(f\"📊 Cluster summary:\")\n",
    "    for cluster_id, count in cluster_counts.items():\n",
    "        if cluster_id != -1:\n",
    "            print(f\"   🏷️  Cluster {cluster_id}: {count} responses ({count/len(cluster_labels)*100:.1f}%)\")\n",
    "    if n_noise > 0:\n",
    "        print(f\"   🔇 Noise: {n_noise} responses ({n_noise/len(cluster_labels)*100:.1f}%)\")\n",
    "else:\n",
    "    print(\"⚠️ No clusters were found in the data\")\n",
    "\n",
    "print(f\"\\n➡️ Ready for Part 4: Summarization and Analysis\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
