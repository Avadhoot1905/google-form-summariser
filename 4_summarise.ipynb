{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3c136e",
   "metadata": {},
   "source": [
    "# Part 4: Summarize Results and Generate Final Output\n",
    "\n",
    "This notebook analyzes the clustered data, generates summaries, and creates final outputs.\n",
    "\n",
    "## Input\n",
    "- `clustered_data.pickle`: Clustered data from Part 3\n",
    "\n",
    "## Output\n",
    "- `final_analysis_results.csv`: Complete results with cluster assignments\n",
    "- `cluster_summary_report.txt`: Detailed analysis report\n",
    "- Word clouds and visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2d7353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages for analysis and visualization\n",
    "!pip install pandas numpy matplotlib seaborn plotly textblob wordcloud\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pickle\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from collections import Counter\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc450b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load clustered data from Part 3\n",
    "print(\"ðŸ“¥ Loading clustered data from Part 3...\")\n",
    "\n",
    "try:\n",
    "    with open('clustered_data.pickle', 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    \n",
    "    df = data['dataframe']\n",
    "    cluster_labels = data['cluster_labels']\n",
    "    clusterer = data['clusterer']\n",
    "    processed_texts = data['processed_texts']\n",
    "    original_texts = data['original_texts']\n",
    "    vectorizer = data['vectorizer']\n",
    "    X_dense = data['tfidf_matrix']\n",
    "    text_columns = data['text_columns']\n",
    "    feature_names = data['feature_names']\n",
    "    n_clusters = data['n_clusters']\n",
    "    n_noise = data['n_noise']\n",
    "    cluster_counts = data['cluster_counts']\n",
    "    \n",
    "    print(f\"âœ… Data loaded successfully!\")\n",
    "    print(f\"ðŸ“Š Total responses: {len(df)}\")\n",
    "    print(f\"ðŸ·ï¸  Clusters found: {n_clusters}\")\n",
    "    print(f\"ðŸ”‡ Noise points: {n_noise}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"âŒ Clustered data file not found. Please run Part 3 first!\")\n",
    "    raise\n",
    "\n",
    "print(f\"\\nðŸ“Š Cluster distribution:\")\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    if cluster_id == -1:\n",
    "        print(f\"   ðŸ”‡ Noise: {count} responses\")\n",
    "    else:\n",
    "        print(f\"   ðŸ·ï¸  Cluster {cluster_id}: {count} responses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be633f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_cluster(cluster_df, cluster_id, vectorizer, X_dense):\n",
    "    \"\"\"Analyze a specific cluster and generate insights\"\"\"\n",
    "    if cluster_id == -1:\n",
    "        return {\"cluster_id\": -1, \"name\": \"Noise\", \"size\": len(cluster_df)}\n",
    "    \n",
    "    cluster_indices = cluster_df.index.tolist()\n",
    "    cluster_texts = [processed_texts[i] for i in cluster_indices]\n",
    "    cluster_original = [original_texts[i] for i in cluster_indices]\n",
    "    \n",
    "    # Get most important terms for this cluster\n",
    "    cluster_vectors = X_dense[cluster_indices]\n",
    "    mean_tfidf = np.mean(cluster_vectors, axis=0)\n",
    "    \n",
    "    # Get top terms\n",
    "    top_indices = np.argsort(mean_tfidf)[-10:][::-1]\n",
    "    top_terms = [(feature_names[i], mean_tfidf[i]) for i in top_indices if mean_tfidf[i] > 0]\n",
    "    \n",
    "    # Sentiment analysis\n",
    "    sentiments = [TextBlob(text).sentiment.polarity for text in cluster_original]\n",
    "    avg_sentiment = np.mean(sentiments)\n",
    "    \n",
    "    # Response length statistics\n",
    "    lengths = [len(text.split()) for text in cluster_original]\n",
    "    \n",
    "    return {\n",
    "        \"cluster_id\": cluster_id,\n",
    "        \"size\": len(cluster_df),\n",
    "        \"top_terms\": top_terms,\n",
    "        \"avg_sentiment\": avg_sentiment,\n",
    "        \"avg_length\": np.mean(lengths),\n",
    "        \"sample_responses\": cluster_original[:3],  # First 3 responses as examples\n",
    "        \"processed_sample\": cluster_texts[:3],\n",
    "        \"all_sentiments\": sentiments\n",
    "    }\n",
    "\n",
    "print(\"ðŸ”§ Analysis functions defined!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c518d520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze each cluster\n",
    "print(\"ðŸ” Analyzing clusters...\")\n",
    "\n",
    "cluster_analyses = []\n",
    "\n",
    "for cluster_id in sorted(set(cluster_labels)):\n",
    "    cluster_mask = df['cluster'] == cluster_id\n",
    "    cluster_data = df[cluster_mask]\n",
    "    \n",
    "    analysis = analyze_cluster(cluster_data, cluster_id, vectorizer, X_dense)\n",
    "    cluster_analyses.append(analysis)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"ðŸ·ï¸  CLUSTER {cluster_id} ANALYSIS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    if cluster_id == -1:\n",
    "        print(f\"ðŸ“‹ Type: Noise/Outliers\")\n",
    "        print(f\"ðŸ“Š Size: {analysis['size']} responses\")\n",
    "    else:\n",
    "        print(f\"ðŸ“Š Size: {analysis['size']} responses ({analysis['size']/len(df)*100:.1f}%)\")\n",
    "        \n",
    "        sentiment_label = ('ðŸ˜Š Positive' if analysis['avg_sentiment'] > 0.1 else \n",
    "                          'ðŸ˜ž Negative' if analysis['avg_sentiment'] < -0.1 else \n",
    "                          'ðŸ˜ Neutral')\n",
    "        print(f\"ðŸ’­ Average Sentiment: {analysis['avg_sentiment']:.3f} ({sentiment_label})\")\n",
    "        print(f\"ðŸ“ Average Response Length: {analysis['avg_length']:.1f} words\")\n",
    "        \n",
    "        print(f\"\\nðŸ”¤ Top Terms:\")\n",
    "        for i, (term, score) in enumerate(analysis['top_terms'][:5]):\n",
    "            print(f\"   {i+1}. {term}: {score:.3f}\")\n",
    "        \n",
    "        print(f\"\\nðŸ’¬ Sample Responses:\")\n",
    "        for i, response in enumerate(analysis['sample_responses']):\n",
    "            print(f\"   {i+1}. {response[:120]}{'...' if len(response) > 120 else ''}\")\n",
    "\n",
    "# Create summary DataFrame\n",
    "summary_data = []\n",
    "for analysis in cluster_analyses:\n",
    "    if analysis['cluster_id'] != -1:\n",
    "        sentiment_label = ('Positive' if analysis['avg_sentiment'] > 0.1 else \n",
    "                          'Negative' if analysis['avg_sentiment'] < -0.1 else \n",
    "                          'Neutral')\n",
    "        \n",
    "        summary_data.append({\n",
    "            'Cluster_ID': analysis['cluster_id'],\n",
    "            'Size': analysis['size'],\n",
    "            'Percentage': f\"{analysis['size']/len(df)*100:.1f}%\",\n",
    "            'Avg_Sentiment': round(analysis['avg_sentiment'], 3),\n",
    "            'Sentiment_Label': sentiment_label,\n",
    "            'Avg_Length': round(analysis['avg_length'], 1),\n",
    "            'Top_3_Terms': ', '.join([term for term, _ in analysis['top_terms'][:3]])\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_data)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸ“Š CLUSTER SUMMARY TABLE\")\n",
    "print(f\"{'='*60}\")\n",
    "if len(summary_df) > 0:\n",
    "    display(summary_df)\n",
    "else:\n",
    "    print(\"âš ï¸ No clusters found for summary table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f3004f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate word clouds for each cluster\n",
    "if n_clusters > 0:\n",
    "    print(\"â˜ï¸ Generating word clouds for clusters...\")\n",
    "    \n",
    "    n_clusters_to_show = min(n_clusters, 6)  # Show up to 6 clusters\n",
    "    n_cols = min(3, n_clusters_to_show)\n",
    "    n_rows = (n_clusters_to_show + n_cols - 1) // n_cols\n",
    "    \n",
    "    if n_clusters_to_show > 0:\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 5*n_rows))\n",
    "        \n",
    "        # Handle single subplot case\n",
    "        if n_clusters_to_show == 1:\n",
    "            axes = [axes]\n",
    "        elif n_rows == 1:\n",
    "            axes = [axes] if n_cols == 1 else axes\n",
    "        else:\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        cluster_counter = 0\n",
    "        \n",
    "        for analysis in cluster_analyses:\n",
    "            if analysis['cluster_id'] != -1 and cluster_counter < n_clusters_to_show:\n",
    "                cluster_id = analysis['cluster_id']\n",
    "                cluster_mask = df['cluster'] == cluster_id\n",
    "                \n",
    "                # Get processed texts for this cluster\n",
    "                cluster_indices = df[cluster_mask].index.tolist()\n",
    "                cluster_texts = [processed_texts[i] for i in cluster_indices]\n",
    "                cluster_text = ' '.join(cluster_texts)\n",
    "                \n",
    "                if cluster_text.strip():\n",
    "                    try:\n",
    "                        wordcloud = WordCloud(\n",
    "                            width=400, \n",
    "                            height=300, \n",
    "                            background_color='white',\n",
    "                            max_words=50,\n",
    "                            colormap='viridis'\n",
    "                        ).generate(cluster_text)\n",
    "                        \n",
    "                        axes[cluster_counter].imshow(wordcloud, interpolation='bilinear')\n",
    "                        axes[cluster_counter].set_title(f'Cluster {cluster_id} Word Cloud\\n({analysis[\"size\"]} responses)')\n",
    "                        axes[cluster_counter].axis('off')\n",
    "                    except ValueError:\n",
    "                        axes[cluster_counter].text(0.5, 0.5, f'Cluster {cluster_id}\\nInsufficient text', \n",
    "                                                 ha='center', va='center', transform=axes[cluster_counter].transAxes)\n",
    "                        axes[cluster_counter].set_title(f'Cluster {cluster_id}')\n",
    "                        axes[cluster_counter].axis('off')\n",
    "                \n",
    "                cluster_counter += 1\n",
    "        \n",
    "        # Hide unused subplots\n",
    "        for i in range(cluster_counter, len(axes)):\n",
    "            axes[i].axis('off')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        print(\"âœ… Word clouds generated!\")\n",
    "    \n",
    "else:\n",
    "    print(\"âš ï¸ No clusters available for word cloud generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e5b3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final results\n",
    "print(\"ðŸ’¾ Exporting final results...\")\n",
    "\n",
    "# Create enhanced DataFrame with all analysis results\n",
    "export_df = df.copy()\n",
    "\n",
    "# Add cluster names based on top terms\n",
    "cluster_names = {}\n",
    "for analysis in cluster_analyses:\n",
    "    if analysis['cluster_id'] != -1:\n",
    "        top_terms = [term for term, _ in analysis['top_terms'][:2]]\n",
    "        cluster_names[analysis['cluster_id']] = f\"Cluster_{analysis['cluster_id']}_{'_'.join(top_terms)}\"\n",
    "    else:\n",
    "        cluster_names[analysis['cluster_id']] = \"Noise\"\n",
    "\n",
    "export_df['cluster_name'] = export_df['cluster'].map(cluster_names)\n",
    "\n",
    "# Add sentiment scores and other metrics\n",
    "export_df['sentiment_score'] = [TextBlob(text).sentiment.polarity for text in original_texts]\n",
    "export_df['response_length'] = [len(text.split()) for text in original_texts]\n",
    "export_df['original_combined_text'] = original_texts\n",
    "export_df['processed_text'] = processed_texts\n",
    "\n",
    "# Export detailed results to CSV\n",
    "output_filename = 'final_analysis_results.csv'\n",
    "export_df.to_csv(output_filename, index=False)\n",
    "print(f\"âœ… Results exported to: {output_filename}\")\n",
    "\n",
    "# Create comprehensive summary report\n",
    "report = f\"\"\"\n",
    "GOOGLE FORM CLUSTERING ANALYSIS - FINAL REPORT\n",
    "=============================================\n",
    "\n",
    "ðŸ“Š DATASET OVERVIEW\n",
    "===================\n",
    "Total responses analyzed: {len(export_df)}\n",
    "Number of clusters found: {n_clusters}\n",
    "Noise points: {n_noise} ({n_noise/len(cluster_labels)*100:.1f}%)\n",
    "\n",
    "ðŸ“ˆ CLUSTERING QUALITY\n",
    "====================\n",
    "\"\"\"\n",
    "\n",
    "# Add silhouette score if available\n",
    "if n_clusters > 1:\n",
    "    from sklearn.metrics import silhouette_score\n",
    "    non_noise_mask = cluster_labels != -1\n",
    "    if np.sum(non_noise_mask) > 1:\n",
    "        silhouette_avg = silhouette_score(X_dense[non_noise_mask], \n",
    "                                        cluster_labels[non_noise_mask])\n",
    "        report += f\"Silhouette Score: {silhouette_avg:.3f}\\n\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "ðŸ·ï¸ DETAILED CLUSTER ANALYSIS\n",
    "============================\n",
    "\"\"\"\n",
    "\n",
    "for analysis in cluster_analyses:\n",
    "    if analysis['cluster_id'] != -1:\n",
    "        sentiment_label = ('Positive' if analysis['avg_sentiment'] > 0.1 else \n",
    "                          'Negative' if analysis['avg_sentiment'] < -0.1 else \n",
    "                          'Neutral')\n",
    "        \n",
    "        report += f\"\"\"\n",
    "Cluster {analysis['cluster_id']}:\n",
    "- Size: {analysis['size']} responses ({analysis['size']/len(export_df)*100:.1f}%)\n",
    "- Average sentiment: {analysis['avg_sentiment']:.3f} ({sentiment_label})\n",
    "- Average response length: {analysis['avg_length']:.1f} words\n",
    "- Key themes: {', '.join([term for term, _ in analysis['top_terms'][:5]])}\n",
    "- Sample response: \"{analysis['sample_responses'][0][:200]}...\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "if n_noise > 0:\n",
    "    report += f\"\"\"\n",
    "Noise/Outliers:\n",
    "- Size: {n_noise} responses ({n_noise/len(export_df)*100:.1f}%)\n",
    "- These are responses that don't fit well into any cluster\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "ðŸ“‹ SUMMARY INSIGHTS\n",
    "==================\n",
    "\"\"\"\n",
    "\n",
    "if n_clusters > 0:\n",
    "    largest_cluster = max([a for a in cluster_analyses if a['cluster_id'] != -1], key=lambda x: x['size'])\n",
    "    most_positive = max([a for a in cluster_analyses if a['cluster_id'] != -1], key=lambda x: x['avg_sentiment'])\n",
    "    most_negative = min([a for a in cluster_analyses if a['cluster_id'] != -1], key=lambda x: x['avg_sentiment'])\n",
    "    \n",
    "    report += f\"\"\"\n",
    "- Largest cluster: Cluster {largest_cluster['cluster_id']} with {largest_cluster['size']} responses\n",
    "- Most positive sentiment: Cluster {most_positive['cluster_id']} (score: {most_positive['avg_sentiment']:.3f})\n",
    "- Most negative sentiment: Cluster {most_negative['cluster_id']} (score: {most_negative['avg_sentiment']:.3f})\n",
    "- Overall data quality: {((len(export_df) - n_noise) / len(export_df) * 100):.1f}% of responses were successfully clustered\n",
    "\"\"\"\n",
    "\n",
    "report += f\"\"\"\n",
    "\n",
    "ðŸ“ OUTPUT FILES\n",
    "===============\n",
    "1. {output_filename} - Complete results with cluster assignments\n",
    "2. cluster_summary_report.txt - This detailed analysis report\n",
    "\n",
    "ðŸš€ NEXT STEPS\n",
    "=============\n",
    "- Review cluster themes and validate with domain knowledge\n",
    "- Use cluster assignments to target specific response groups\n",
    "- Consider sub-clustering large clusters for deeper insights\n",
    "- Apply findings to improve future surveys or processes\n",
    "\n",
    "Generated on: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "\"\"\"\n",
    "\n",
    "# Save report\n",
    "report_filename = 'cluster_summary_report.txt'\n",
    "with open(report_filename, 'w') as f:\n",
    "    f.write(report)\n",
    "\n",
    "print(f\"âœ… Analysis report saved to: {report_filename}\")\n",
    "print(f\"\\nðŸŽ‰ ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
    "print(f\"ðŸ“Š Summary:\")\n",
    "print(f\"   ðŸ“‹ Total responses: {len(export_df)}\")\n",
    "print(f\"   ðŸ·ï¸  Clusters found: {n_clusters}\")\n",
    "print(f\"   ðŸ’¾ Files generated:\")\n",
    "print(f\"      - {output_filename}\")\n",
    "print(f\"      - {report_filename}\")\n",
    "\n",
    "if len(summary_df) > 0:\n",
    "    print(f\"\\nðŸ“Š Final Summary Table:\")\n",
    "    display(summary_df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
